{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HardTables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class CEA_Evaluator:\n",
    "    def __init__(self, answer_file_path, round=1):\n",
    "        \"\"\"\n",
    "    `round` : Holds the round for which the evaluation is being done. \n",
    "    can be 1, 2...upto the number of rounds the challenge has.\n",
    "    Different rounds will mostly have different ground truth files.\n",
    "    \"\"\"\n",
    "        self.answer_file_path = answer_file_path\n",
    "        self.round = round\n",
    "\n",
    "    def _evaluate(self, client_payload, _context={}):\n",
    "        \"\"\"\n",
    "    `client_payload` will be a dict with (atleast) the following keys :\n",
    "      - submission_file_path : local file path of the submitted file\n",
    "      - aicrowd_submission_id : A unique id representing the submission\n",
    "      - aicrowd_participant_id : A unique id for participant/team submitting (if enabled)\n",
    "    \"\"\"\n",
    "        submission_file_path = client_payload[\"submission_file_path\"]\n",
    "\n",
    "        gt_cell_ent = dict()\n",
    "        gt = pd.read_csv(self.answer_file_path, delimiter=',', names=['tab_id', 'col_id', 'row_id', 'entity'],\n",
    "                         dtype={'tab_id': str, 'col_id': str, 'row_id': str, 'entity': str},header= None, keep_default_na=False,nrows=1000)\n",
    "\n",
    "        for index, row in gt.iterrows():\n",
    "            cell = '%s %s %s' % (row['tab_id'], row['col_id'], row['row_id'])\n",
    "            gt_cell_ent[cell] = row['entity']\n",
    "\n",
    "        correct_cells, annotated_cells = set(), set()\n",
    "        sub = pd.read_csv(submission_file_path, delimiter=',', names=['tab_id', 'col_id', 'row_id','entity'],\n",
    "                          dtype={'tab_id': str, 'col_id': str, 'row_id': str, 'entity': str}, keep_default_na=False)\n",
    "        \n",
    "        for index, row in sub.iterrows():\n",
    "            cell = '%s %s %s' % (row['tab_id'], row['col_id'], row['row_id'])\n",
    "            if cell in gt_cell_ent:\n",
    "            \n",
    "                if cell in annotated_cells:\n",
    "                    raise Exception(\"Duplicate cells in the submission file\")\n",
    "                else:\n",
    "                    annotated_cells.add(cell)\n",
    "\n",
    "                annotation = row['entity']\n",
    "                if not annotation.lower() == 'nil' and not annotation.startswith('http://www.wikidata.org/entity/'):\n",
    "                    annotation = 'http://www.wikidata.org/entity/' + annotation\n",
    "\n",
    "                if annotation.lower() in gt_cell_ent[cell].lower().split():\n",
    "                    correct_cells.add(cell)\n",
    "                else:\n",
    "                    print('%s,%s' % (cell.replace(' ', ','), gt_cell_ent[cell]))\n",
    "        \n",
    "        if len(annotated_cells) > 0:\n",
    "            precision = len(correct_cells) / len(annotated_cells)\n",
    "        else:\n",
    "            precision = 0.0\n",
    "        \n",
    "        recall = len(correct_cells) / len(gt_cell_ent.keys())\n",
    "    \n",
    "        if (precision + recall) > 0:\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            f1= 0.0\n",
    "        \n",
    "        main_score = f1\n",
    "        secondary_score = precision\n",
    "        print('F1: %.10f, Precision: %.10f, Recall: %.10f' % (f1, precision, recall))\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "    Do something with your submitted file to come up\n",
    "    with a score and a secondary score.\n",
    "\n",
    "    if you want to report back an error to the user,\n",
    "    then you can simply do :\n",
    "      `raise Exception(\"YOUR-CUSTOM-ERROR\")`\n",
    "\n",
    "     You are encouraged to add as many validations as possible\n",
    "     to provide meaningful feedback to your users\n",
    "    \"\"\"\n",
    "        _result_object = {\n",
    "            \"F1\": main_score,\n",
    "            \"Precision\": secondary_score,\n",
    "            \"Recall\": recall\n",
    "        }\n",
    "        return _result_object\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Lets assume the ground_truth is a CSV file\n",
    "    # and is present at data/ground_truth.csv\n",
    "    # and a sample submission is present at data/sample_submission.csv\n",
    "    answer_file_path = \"../input/HardTables/HT_gt_WD.csv\"\n",
    "    \n",
    "    d = '../evaluation/prediction_submissions/HardTables'\n",
    "    file_resultsHT = {}  # Dictionary to store filename and result pairs\n",
    "\n",
    "    for ff in os.listdir(d):\n",
    "        _client_payload = {}\n",
    "        if ff == '.DS_Store':\n",
    "            continue\n",
    "        print(ff)\n",
    "        _client_payload[\"submission_file_path\"] = os.path.join(d, ff)\n",
    "\n",
    "        # Instantiate a dummy context\n",
    "        _context = {}\n",
    "        # Instantiate an evaluator\n",
    "        cea_evaluator = CEA_Evaluator(answer_file_path)\n",
    "        # Evaluate\n",
    "        result = cea_evaluator._evaluate(_client_payload, _context)\n",
    "        file_resultsHT[ff] = result  # Store the result for each file in the dictionary\n",
    "\n",
    "    # Print filename and result for each file after the loop\n",
    "    for filename, result in file_resultsHT.items():\n",
    "        print(f\"File: {filename}, Result: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame.from_dict(file_resultsHT, orient='index')\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df1.to_excel('../evaluation/excel_results/HT_Results.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToughTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class CEA_Evaluator:\n",
    "    def __init__(self, answer_file_path, round=1):\n",
    "        \"\"\"\n",
    "    `round` : Holds the round for which the evaluation is being done. \n",
    "    can be 1, 2...upto the number of rounds the challenge has.\n",
    "    Different rounds will mostly have different ground truth files.\n",
    "    \"\"\"\n",
    "        self.answer_file_path = answer_file_path\n",
    "        self.round = round\n",
    "\n",
    "    def _evaluate(self, client_payload, _context={}):\n",
    "        \"\"\"\n",
    "    `client_payload` will be a dict with (atleast) the following keys :\n",
    "      - submission_file_path : local file path of the submitted file\n",
    "      - aicrowd_submission_id : A unique id representing the submission\n",
    "      - aicrowd_participant_id : A unique id for participant/team submitting (if enabled)\n",
    "    \"\"\"\n",
    "        submission_file_path = client_payload[\"submission_file_path\"]\n",
    "\n",
    "        gt_cell_ent = dict()\n",
    "        gt = pd.read_csv(self.answer_file_path, delimiter=',', names=['tab_id', 'col_id', 'row_id', 'entity'],\n",
    "                         dtype={'tab_id': str, 'col_id': str, 'row_id': str, 'entity': str},header= None, keep_default_na=False,nrows=1000)\n",
    "\n",
    "        for index, row in gt.iterrows():\n",
    "            cell = '%s %s %s' % (row['tab_id'], row['col_id'], row['row_id'])\n",
    "            gt_cell_ent[cell] = row['entity']\n",
    "\n",
    "        correct_cells, annotated_cells = set(), set()\n",
    "        sub = pd.read_csv(submission_file_path, delimiter=',', names=['tab_id','col_id', 'row_id','entity'],\n",
    "                          dtype={'tab_id': str, 'row_id': str, 'col_id': str, 'entity': str}, keep_default_na=False)\n",
    "        \n",
    "        for index, row in sub.iterrows():\n",
    "            cell = '%s %s %s' % (row['tab_id'], row['col_id'], row['row_id'])\n",
    "            if cell in gt_cell_ent:\n",
    "            \n",
    "                if cell in annotated_cells:\n",
    "                    raise Exception(\"Duplicate cells in the submission file\")\n",
    "                else:\n",
    "                    annotated_cells.add(cell)\n",
    "\n",
    "                annotation = row['entity']\n",
    "                if not annotation.lower() == 'nil' and not annotation.startswith('http://www.wikidata.org/entity/'):\n",
    "                    annotation = 'http://www.wikidata.org/entity/' + annotation\n",
    "\n",
    "                if annotation.lower() in gt_cell_ent[cell].lower().split():\n",
    "                    \n",
    "                    correct_cells.add(cell)\n",
    "                else:\n",
    "                    print('%s,%s' % (cell.replace(' ', ','), gt_cell_ent[cell]))\n",
    "        \n",
    "        if len(annotated_cells) > 0:\n",
    "            precision = len(correct_cells) / len(annotated_cells)\n",
    "        else:\n",
    "            precision = 0.0\n",
    "        \n",
    "        recall = len(correct_cells) / len(gt_cell_ent.keys())\n",
    "    \n",
    "        if (precision + recall) > 0:\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            f1= 0.0\n",
    "    \n",
    "        main_score = f1\n",
    "        secondary_score = precision\n",
    "        print('F1: %.10f, Precision: %.10f, Recall: %.10f' % (f1, precision, recall))\n",
    "        print(len(gt_cell_ent.keys()),\" \",len(correct_cells))\n",
    "\n",
    "        \"\"\"\n",
    "    Do something with your submitted file to come up\n",
    "    with a score and a secondary score.\n",
    "\n",
    "    if you want to report back an error to the user,\n",
    "    then you can simply do :\n",
    "      `raise Exception(\"YOUR-CUSTOM-ERROR\")`\n",
    "\n",
    "     You are encouraged to add as many validations as possible\n",
    "     to provide meaningful feedback to your users\n",
    "    \"\"\"\n",
    "        _result_object = {\n",
    "            \"F1\": main_score,\n",
    "            \"Precision\": secondary_score,\n",
    "            \"Recall\": recall\n",
    "        }\n",
    "        return _result_object\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Lets assume the ground_truth is a CSV file\n",
    "    # and is present at data/ground_truth.csv\n",
    "    # and a sample submission is present at data/sample_submission.csv\n",
    "    answer_file_path = \"../input/ToughTables/2T_gt_WD.csv\"\n",
    "    \n",
    "    d = '../evaluation/prediction_submissions/ToughTables'\n",
    "    file_results2T = {}  # Dictionary to store filename and result pairs\n",
    "\n",
    "    for ff in os.listdir(d):\n",
    "        _client_payload = {}\n",
    "        if ff == '.DS_Store':\n",
    "            continue\n",
    "        print(ff)\n",
    "        _client_payload[\"submission_file_path\"] = os.path.join(d, ff)\n",
    "\n",
    "        # Instantiate a dummy context\n",
    "        _context = {}\n",
    "        # Instantiate an evaluator\n",
    "        cea_evaluator = CEA_Evaluator(answer_file_path)\n",
    "        # Evaluate\n",
    "        result = cea_evaluator._evaluate(_client_payload, _context)\n",
    "        file_results2T[ff] = result  # Store the result for each file in the dictionary\n",
    "\n",
    "    # Print filename and result for each file after the loop\n",
    "    for filename, result in file_results2T.items():\n",
    "        print(f\"File: {filename}, Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame.from_dict(file_results2T, orient='index')\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df2.to_excel('../evaluation/excel_results/2T_Results.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
